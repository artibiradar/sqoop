LIST DATABASES
-------------
[hduser@ubuntu ~]$ sqoop list-databases --connect
jdbc:mysql://localhost --username root --password '';

LIST TABLES in a database
--------------------------
[hduser@ubuntu ~]$ sqoop list-tables --connect
jdbc:mysql://localhost/college --username root --password '';








Import one table (with key)from mysql into HDFS
------------------------------------------------
[hduser@ubuntu ~]$ sqoop import --connect
jdbc:mysql://localhost/college --username root --password '' --table
student_master --target-dir /hanoi/student_master;

Add an extra record in mysql in college db
INSERT INTO student_master
     (name, address)
     VALUES
    ("New", "New Place");
WITH INCREMENTAL
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --check-column student_id
--incremental append --last-value  8 --target-dir
/niit/student_master;

COMPRESS FILE
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --target-dir
/niit/student_master1 --compress -m 1;

Import one table (without key)from mysql into HDFS
------------------------------------------------
[hduser@ubuntu ~]$ sqoop import --connect
jdbc:mysql://localhost/college --username root --password '' --table
topten --target-dir /niit/topten -m 1 ;

import table to avro type or a sequence
---------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table topten --target-dir /niit/top10avro
--as-avrodatafile -m 1 ;
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table topten --target-dir /niit/top10seq
--as-sequencefile -m 1 ;

create avro table and load data in hive
CREATE TABLE toptenavro
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES (
    'avro.schema.literal'='{
      "namespace": "abc",
      "name": "top10",
      "type": "record",
      "fields": [
{ "name":"customer_id","type":"int"}, {
"name":"fname","type":"string"}, { "name":"lname","type":"string"},{
"name":"age","type":"int"},{ "name":"profession","type":"string"},{
"name":"amount","type":"double"}]
    }');

load data inpath '/niit/top10avro/part-m-00000.avro' overwrite into
table toptenavro;


WITH WHERE CLAUSE
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --where 'student_id=1 or
student_id=3' --target-dir /niit/query -m 1;
WITH QUERY
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select * from student_master where $CONDITIONS
and student_id=2' --target-dir /niit/query1 -m 1;



WITH INNER JOINS in form of Query
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select a.student_id, a.name, a.address,
b.result from student_master a, fy b where $CONDITIONS and
a.student_id=b.student_id' --target-dir /niit/query2 -m 1;


WITH LEFT OUTER JOINS in form of Query
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select a.student_id, a.name, a.address,
b.result from
student_master a left outer join fy b on a.student_id = b.student_id
where $CONDITIONS' --target-dir /niit/query3 -m 1;

WITH RIGHT OUTER JOINS in form of Query
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select a.student_id, a.name, a.address,
b.result from
student_master a right outer join fy b on a.student_id = b.student_id
where $CONDITIONS' --target-dir /niit/query4 -m 1;




WITH COLUMN CLAUSE
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --columns "student_id,name"
--target-dir /niit/query5 -m 1;



Import all tables from mysql into hdfs
----------------------------------------------------
[hduser@ubuntu ~]$ sqoop import-all-tables --connect
jdbc:mysql://localhost/college --username root --password ''
--warehouse-dir /niit/all_tables  ;


Import data into hive managed tables
-----------------------------------------------------
First create a database college in hive
hive> create database college;
hive> quit;
sqoop import --connect jdbc:mysql://localhost/college --username  root
--password '' --table student_master --hive-import --hive-table
college.student_profile -m 1;